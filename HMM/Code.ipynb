{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import DM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(file_path, filename):\n",
    "    with open(file_path + filename, 'r') as file:\n",
    "        data = file.readlines()\n",
    "    data = [line.replace('\\n', '') for line in data]\n",
    "    data = [line.split() for line in data]\n",
    "    data = np.array([list(map(float, line)) for line in data])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "file_path = os.getcwd() + '/data/'\n",
    "file_train = \"EMGaussian.data\"\n",
    "file_test = \"EMGaussian.test\"\n",
    "train = to_numpy(file_path, file_train)\n",
    "test = to_numpy(file_path, file_test)\n",
    "data_set = {'train': train,\n",
    "            'test': test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnpuq(ut, qt, mu, cov):\n",
    "    r=np.log(multivariate_normal(mu[qt], sigma_square[qt]).pdf(ut))\n",
    "    print('r= ' , r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(data, pi, mu, sigma_square, logA):\n",
    "    # forward recursion\n",
    "    K=np.shape(logA)[0]\n",
    "    N = data.shape[0]\n",
    "    logalpha = np.zeros((N,K))\n",
    "    logalpha[0,:] = [np.log(pi[k]) + np.log(multivariate_normal(mu[k], sigma_square[k]).pdf(data[0])) \\\n",
    "                     for k in range(K)]\n",
    "    \n",
    "    #We use the logarithm technique seen in course  \n",
    "    for n in range(1,N):\n",
    "        for k in range(K):\n",
    "            listalpha = np.array([logalpha[n-1,i] + logA[i,k] for i in range(K)])\n",
    "            alphamax = np.max(listalpha)                  \n",
    "            logalpha[n,k] = np.log(multivariate_normal(mu[k], sigma_square[k]).pdf(data[n])) + \\\n",
    "                            alphamax + logsumexp(listalpha-alphamax)\n",
    "                                   \n",
    "    # backward recursion\n",
    "    logbeta = np.zeros((N,K))\n",
    "    #We use the logarithm technique seen in course  \n",
    "    for n in range(N-2,-1,-1):\n",
    "        for k in range(K):\n",
    "            listbeta = np.array([logbeta[n+1,i] + logA[k,i] + \\\n",
    "                                np.log(multivariate_normal(mu[i], sigma_square[i]).pdf(data[n+1])) \\\n",
    "                               for i in range(K)])\n",
    "            betamax = np.max(listbeta)\n",
    "            logbeta[n,k] = betamax + logsumexp(listbeta-betamax)\n",
    "    \n",
    "    return (logalpha, logbeta)\n",
    "\n",
    "def log_zeta(data, logalpha, logbeta, mu, sigma_square, logA):\n",
    "    # un-normalized loggamma i.e log(p(q_t,u_0,...,u_T))\n",
    "    #We start by calculating logp(u_0,...,u_T) and we use the same technique of log by writing p(U)=sum over q_0 p(q_0,U)\n",
    "    listnorm= logalpha[-1,:]\n",
    "    maxn=np.max(listnorm)\n",
    "    norm=maxn+logsumexp(listnorm-maxn)\n",
    "    \n",
    "    #it's a three dimenstional matrix , at each time we have a KxK matrix \n",
    "    N=np.shape(data)[0]\n",
    "    K=np.shape(logA)[0]\n",
    "    logzeta=np.zeros((N-1,K,K))\n",
    "    \n",
    "    for n in range(1,N):\n",
    "        for k in range(K):\n",
    "            logzeta[n-1,:,k]= [np.log(multivariate_normal(mu[k], sigma_square[k]).pdf(data[n])) + \\\n",
    "                                logalpha[n-1,i] + logbeta[n,k] + logA[i,k] - norm for i in range(K)]\n",
    "    return logzeta\n",
    "\n",
    "def log_gamma(logalpha, logbeta):\n",
    "    # un-normalized loggamma i.e log(p(q_t,u_0,...,u_T))\n",
    "    loggamma = logalpha+logbeta\n",
    "    #let's calculate normalized loggamma i.e log(p(q_t/u_0,......,u_T))\n",
    "    #We start by calculating logp(u_0,...,u_T) and we use the same technique of log by writing p(U)=sum over q_0 p(q_0,U)\n",
    "    listnorm = loggamma[0,:]\n",
    "    maxn = np.max(listnorm)\n",
    "    norm = maxn + logsumexp(listnorm-maxn)\n",
    "    loggamma_norm = loggamma - norm\n",
    "    return loggamma_norm\n",
    "\n",
    "def update_parameters(data, logalpha, logbeta, pi, mu, sigma_square, logA):\n",
    "    N = data.shape[0]\n",
    "    K = pi.shape[0]\n",
    "    \n",
    "    loggamma = log_gamma(logalpha, logbeta)\n",
    "    logzeta = log_zeta(data, logalpha, logbeta, mu, sigma_square, logA)\n",
    "    \n",
    "    # estimate log transition matrix A\n",
    "    for i in range(K):\n",
    "        norm = logsumexp(logzeta[:,i,:])\n",
    "        logA[i,:] = [logsumexp(logzeta[:, i, k]) - norm for k in range(K)]\n",
    "        \n",
    "    # estimate initial distribution pi, and mu, sigma:\n",
    "    pi = np.exp(loggamma[0,:])\n",
    "    for k in range(K):\n",
    "        weights = np.array([1 / np.sum(np.exp(loggamma[:,k] - loggamma[n,k])) for n in range(N)])\n",
    "        mu[k] = np.average(data, axis=0, weights=weights)\n",
    "        # or use: mu[k] = np.multiply(weights.reshape(-1,1), data).sum(axis = 0)\n",
    "        sigma_square[k] = (data - mu[k]).T.dot(np.multiply(weights.reshape(-1,1), \\\n",
    "                          (data - mu[k])))\n",
    "\n",
    "    # calculate loglikelihood of complete data.\n",
    "    term_1 = np.exp(loggamma[0,:]).dot(loggamma[0,:]) \n",
    "    term_2 = (np.exp(logzeta).sum(axis = 0) * logA).sum()        \n",
    "    term_3 = sum(np.exp(loggamma[:,k]).dot(np.log(multivariate_normal(mu[k], sigma_square[k]).pdf(data))) \\\n",
    "                 for k in range(K))\n",
    "    loglikelihood = term_1 + term_2 + term_3\n",
    "\n",
    "    return (pi, mu, sigma_square, logA, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM(data, n_groups, epsilon, n_iter_max, init_param):\n",
    "    \n",
    "    K = n_groups\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    # init\n",
    "    old_pi = init_param['pi']\n",
    "    old_mu = init_param['mu']\n",
    "    old_sigma_square = init_param['sigma_square']\n",
    "    old_logA = np.log(np.ones((K,K))/K)\n",
    "    \n",
    "    logalpha, logbeta = forward_backward(data, old_pi, old_mu, old_sigma_square, old_logA)\n",
    "    new_pi, new_mu, new_sigma_square, new_logA, new_loglikelihood = \\\n",
    "            update_parameters(data, logalpha, logbeta, old_pi, old_mu, old_sigma_square, old_logA)\n",
    "    relative_error = epsilon\n",
    "    count = 0\n",
    "    \n",
    "    while abs(relative_error) >= epsilon and count <= n_iter_max:\n",
    "        old_pi, old_mu, old_sigma_square, old_logA, old_loglikelihood = \\\n",
    "                        new_pi, new_mu, new_sigma_square, new_logA, new_loglikelihood\n",
    "        logalpha, logbeta = forward_backward(data, old_pi, old_mu, old_sigma_square, old_logA)\n",
    "        new_pi, new_mu, new_sigma_square, new_logA, new_loglikelihood = \\\n",
    "                        update_parameters(data, logalpha, logbeta, old_pi, old_mu, old_sigma_square, old_logA)\n",
    "        relative_error = new_loglikelihood/old_loglikelihood-1\n",
    "        count += 1\n",
    "    \n",
    "    print('Converge after', count, 'iterations')\n",
    "        \n",
    "    return (new_pi, new_mu, new_sigma_square, new_logA, new_loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, pi, mu, sigma_square, logA):\n",
    "    # implementation of Viterbi algo\n",
    "    \n",
    "    K = logA.shape[0]\n",
    "    N = data.shape[0]    \n",
    "    logalpha = np.zeros((N,K))\n",
    "    qmaxalpha = np.zeros((N, K), dtype=int)\n",
    "    for k in range(K):\n",
    "        logalpha[0,k]=np.log(pi[k])+lnpuq(data[0,:], k, mu, sigma_square)\n",
    "    print(logalpha[0,:])\n",
    "    for n in range(1, N):\n",
    "        for k in range(K):\n",
    "            proba=[logA[i,k] + np.log(multivariate_normal(mu[i], sigma_square[i]).pdf(data[n-1])) + \\\n",
    "                                logalpha[n, i] for i in range(K)]\n",
    "            logalpha[n,k] = max(proba)\n",
    "            qmaxalpha[n,k]=np.argmax(proba)       \n",
    "    \n",
    "    \n",
    "    \n",
    "    q_max = np.zeros(N, dtype=int)\n",
    "        \n",
    "    q_max[N-1] = np.argmax(logalpha[N-1, :])\n",
    "    \n",
    "    for n in range(1, N):\n",
    "        n_next = N - n\n",
    "        q_max[n_next - 1] = qmaxalpha[n_next, q_max[n_next]]\n",
    "    \n",
    "    return q_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "n_iter_max = 100\n",
    "K = 4\n",
    "mu_EM, sigma_square_EM, pi_EM, _ = DM2.EM(data_set['train'], K, epsilon, 100, 5, 100, False)\n",
    "\n",
    "init_param = {'pi': pi_EM,\n",
    "             'mu': mu_EM,\n",
    "             'sigma_square': sigma_square_EM}\n",
    "\n",
    "pi, mu, sigma_square, logA, loglikelihood = HMM(data_set['train'], K, epsilon, n_iter_max, init_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"orange\", \"blue\", \"brown\", \"pink\"]\n",
    "\n",
    "fig = plt.figure(figsize=(30,12))\n",
    "ax = fig.add_subplot(1,2, 1)\n",
    "\n",
    "# plot EM HMM\n",
    "clusters_HMM = get_clusters(data_set['train'], pi, mu, sigma_square, logA)\n",
    "for k,color in enumerate(colors):\n",
    "    # generate the ellipsoid corresponding to the parameters\n",
    "    points = np.random.multivariate_normal(mean=mu[k], cov=sigma_square[k], size=100000)\n",
    "    x, y = points.T\n",
    "    ax.plot(x, y, ls = '', marker = 'o', ms = 0.000001, color = 'white')\n",
    "    DM2.plot_point_cov(points, nstd=2, alpha=0.5, color='green')\n",
    "\n",
    "    # plot data in each group\n",
    "    group = train[clusters_HMM == k]\n",
    "    ax.plot(group[:,0], group[:,1], ls = '', marker = 'o', \\\n",
    "                ms = 5, color = color, label = 'cluster ' + str(k+1))\n",
    "\n",
    "ax.set_title('EM HMM', fontsize = 18)\n",
    "ax.set_xlim((min(train[:,0]) - 2, max(train[:,0]) + 2))\n",
    "ax.set_ylim((min(train[:,1]) - 2, max(train[:,1]) + 2))\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14)        \n",
    "plt.legend(fontsize = 14)\n",
    "plt.grid()\n",
    "\n",
    "# plot EM GMM\n",
    "\n",
    "matrix_likelihood = DM2.mixture_density(pi_EM, mu_EM, sigma_square_EM, train, K)\n",
    "clusters_GMM = np.argmax(matrix_likelihood, axis = 1)\n",
    "\n",
    "ax = fig.add_subplot(1,2, 2)\n",
    "for k,color in enumerate(colors):\n",
    "    # generate the ellipsoid corresponding to the parameters\n",
    "    points = np.random.multivariate_normal(mean=mu_EM[k], cov=sigma_square_EM[k], size=100000)\n",
    "    x, y = points.T\n",
    "    ax.plot(x, y, ls = '', marker = 'o', ms = 0.000001, color = 'white')\n",
    "    DM2.plot_point_cov(points, nstd=2, alpha=0.5, color='green')\n",
    "\n",
    "    # plot data in each group\n",
    "    group = train[clusters_GMM == k]\n",
    "    ax.plot(group[:,0], group[:,1], ls = '', marker = 'o', \\\n",
    "            ms = 5, color = color, label = 'cluster ' + str(k+1))\n",
    "\n",
    "ax.set_title('EM GMM', fontsize = 18)\n",
    "ax.set_xlim((min(train[:,0]) - 2, max(train[:,0]) + 2))\n",
    "ax.set_ylim((min(train[:,1]) - 2, max(train[:,1]) + 2))\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(14)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(14)        \n",
    "plt.legend(fontsize = 14)\n",
    "plt.grid()\n",
    "\n",
    "fig.savefig(os.path.join(os.getcwd(), 'Plot2'))\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loglikelihood(data, logalpha, logbeta, pi, mu, sigma_square, logA):\n",
    "    N = data.shape[0]\n",
    "    K = pi.shape[0]\n",
    "    \n",
    "    loggamma = log_gamma(logalpha, logbeta)\n",
    "    logzeta = log_zeta(data, logalpha, logbeta, mu, sigma_square, logA)\n",
    "    \n",
    "    # estimate log transition matrix A\n",
    "    for i in range(K):\n",
    "        norm = logsumexp(logzeta[:,i,:])\n",
    "        logA[i,:] = [logsumexp(logzeta[:, i, k]) - norm for k in range(K)]\n",
    "\n",
    "    # calculate loglikelihood of complete data.\n",
    "    term_1 = np.exp(loggamma[0,:]).dot(loggamma[0,:]) \n",
    "    term_2 = (np.exp(logzeta).sum(axis = 0) * logA).sum()        \n",
    "    term_3 = sum(np.exp(loggamma[:,k]).dot(np.log(multivariate_normal(mu[k], sigma_square[k]).pdf(data))) \\\n",
    "                 for k in range(K))\n",
    "    loglikelihood = term_1 + term_2 + term_3\n",
    "\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logalpha, logbeta = forward_backward(data_set['test'], pi, mu, sigma_square, logA)\n",
    "loglikelihood = calculate_loglikelihood(data_set['test'], logalpha, logbeta, pi, mu, sigma_square, logA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
